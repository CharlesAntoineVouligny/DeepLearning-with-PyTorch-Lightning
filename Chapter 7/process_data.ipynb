{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "363f3e0d",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import json\n",
    "import nltk\n",
    "import pickle\n",
    "from collections import Counter\n",
    "\n",
    "from PIL import Image\n",
    "\n",
    "from vocabulary import Vocabulary"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "87173b5f",
   "metadata": {},
   "outputs": [],
   "source": [
    "def resize_images(input_path, output_path, new_size):\n",
    "    if not os.path.exists(output_path):\n",
    "        os.makedirs(output_path)\n",
    "\n",
    "    image_files = os.listdir(input_path)\n",
    "    num_images = len(image_files)\n",
    "    for i, img in enumerate(image_files):\n",
    "        img_full_path = os.path.join(input_path, img)\n",
    "        with open(img_full_path, 'r+b') as f:\n",
    "            with Image.open(f) as image:\n",
    "                image = image.resize(new_size, Image.ANTIALIAS)\n",
    "                img_sv_full_path = os.path.join(output_path, img)\n",
    "                image.save(img_sv_full_path, image.format)\n",
    "        if (i+1) % 100 == 0 or (i+1) == num_images:\n",
    "            print(\"Resized {} out of {} total images.\".format(i+1, num_images))\n",
    "\n",
    "input_path = './coco_data/images/'\n",
    "output_path = './coco_data/resized_images/'\n",
    "new_size = [256, 256]\n",
    "resize_images(input_path, output_path, new_size)\n",
    "\n",
    "!rm -rf ./coco_data/images\n",
    "!mv ./coco_data/resized_images ./coco_data/images"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ddd9f94d",
   "metadata": {},
   "outputs": [],
   "source": [
    "def build_vocabulary(json_path, threshold):\n",
    "  with open(json_path) as json_file:\n",
    "    captions = json.load(json_file)\n",
    "  counter = Counter()\n",
    "  i = 0\n",
    "  for annotation in captions['annotations']:\n",
    "    i = i + 1\n",
    "    caption = annotation['caption']\n",
    "    tokens = nltk.tokenize.word_tokenize(caption.lower())\n",
    "    counter.update(tokens)\n",
    "    if i % 1000 == 0 or i == len(captions['annotations']):\n",
    "      print(\"Tokenized {} out of total {} captions.\".format(i, len(captions['annotations'])))\n",
    "\n",
    "  # If the word frequency is less than 'threshold', then the word is discarded.\n",
    "  tokens = [tkn for tkn, i in counter.items() if i >= threshold]\n",
    "\n",
    "  # Create a vocabulary wrapper and add some special tokens.\n",
    "  vocabulary = Vocabulary()\n",
    "  vocabulary.add_token('<pad>')\n",
    "  vocabulary.add_token('<start>')\n",
    "  vocabulary.add_token('<end>')\n",
    "  vocabulary.add_token('<unk>')\n",
    "\n",
    "  # Add the words to the vocabulary.\n",
    "  for i, token in enumerate(tokens):\n",
    "    vocabulary.add_token(token)\n",
    "  return vocabulary\n",
    "\n",
    "vocabulary = build_vocabulary(json_path='coco_data/captions.json', threshold=4)\n",
    "vocabulary_path = './coco_data/vocabulary.pkl'\n",
    "with open(vocabulary_path, 'wb') as f:\n",
    "    pickle.dump(vocabulary, f)\n",
    "print(\"Total vocabulary size: {}\".format(len(vocabulary)))"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
